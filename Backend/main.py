# -*- coding: utf-8 -*-
"""maestro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HPxhXEZxbgU8vLi9cGpSFxlxtLRwSqu2
"""

# Install required libraries
!pip install torch torchvision torchaudio --quiet
!pip install pretty_midi numpy tqdm pyfluidsynth --quiet

# Install FluidSynth synthesizer for audio rendering
!sudo apt-get install -y fluidsynth

import fluidsynth
print("✅ fluidsynth (pyFluidSynth) is working and ready to use!")

!pip install pretty_midi

import pretty_midi
import os

# Define Ragas by their intervals from the tonic (Sa)
RAGAS = {
    'mayamalavagowla': {'swaras': [0, 1, 4, 5, 7, 8, 11, 12]},
    'sankarabharanam': {'swaras': [0, 2, 4, 5, 7, 9, 11, 12]},
    'kalyani': {'swaras': [0, 2, 4, 6, 7, 9, 11, 12]},
    'mohanam': {'swaras': [0, 2, 4, 7, 9, 12]} # Pentatonic
}

MIDI_OUTPUT_DIR = 'data/midi'
os.makedirs(MIDI_OUTPUT_DIR, exist_ok=True)
BASE_NOTE = 60 # C4
NOTE_DURATION = 0.3

def write_midi_file(notes, filename, instrument_name='Acoustic Grand Piano'):
    """Takes a list of note events and writes them to a MIDI file."""
    filepath = os.path.join(MIDI_OUTPUT_DIR, filename)
    midi = pretty_midi.PrettyMIDI()
    instrument = pretty_midi.Instrument(program=pretty_midi.instrument_name_to_program(instrument_name))
    instrument.notes.extend(notes)
    midi.instruments.append(instrument)
    midi.write(filepath)
    print(f"Generated: {filename}")

def generate_scale_midi(raga_name, swaras, repeats=3):
    """Generates a MIDI file by repeating the raga's scale."""
    notes = []
    current_time = 0.0
    for _ in range(repeats):
        # Ascending (Arohanam)
        for interval in swaras[:-1]:
            note = pretty_midi.Note(velocity=100, pitch=BASE_NOTE + interval, start=current_time, end=current_time + NOTE_DURATION)
            notes.append(note)
            current_time += NOTE_DURATION
        # Descending (Avarohanam)
        for interval in reversed(swaras):
            note = pretty_midi.Note(velocity=100, pitch=BASE_NOTE + interval, start=current_time, end=current_time + NOTE_DURATION)
            notes.append(note)
            current_time += NOTE_DURATION
        current_time += NOTE_DURATION # Pause between repeats
    write_midi_file(notes, f"{raga_name}_adi_scales.mid")

def generate_alankara_midi(raga_name, swaras):
    """Generates a MIDI file with a simple melodic pattern (SR, RG, GM...)."""
    notes = []
    current_time = 0.0
    # Pattern: S R S, R G R, G M G ...
    for i in range(len(swaras) - 2):
        pattern = [swaras[i], swaras[i+1], swaras[i]]
        for interval in pattern:
            note = pretty_midi.Note(velocity=100, pitch=BASE_NOTE + interval, start=current_time, end=current_time + NOTE_DURATION)
            notes.append(note)
            current_time += NOTE_DURATION
        current_time += NOTE_DURATION * 0.5 # Small pause
    write_midi_file(notes, f"{raga_name}_adi_alankara.mid")

# --- Main Generation ---
# First, clear out any old files
for file in os.listdir(MIDI_OUTPUT_DIR):
    os.remove(os.path.join(MIDI_OUTPUT_DIR, file))

# Generate new, longer files
for raga, data in RAGAS.items():
    generate_scale_midi(raga, data['swaras'])
    generate_alankara_midi(raga, data['swaras'])

print(f"\n✅ Successfully created a new, longer dataset in '{MIDI_OUTPUT_DIR}'.")

import pretty_midi
import os
import random

# --- Configuration ---
# Define the Ragas and Talas for the dataset
RAGAS = {
    'mayamalavagowla': {'swaras': [0, 1, 4, 5, 7, 8, 11, 12]},
    'sankarabharanam': {'swaras': [0, 2, 4, 5, 7, 9, 11, 12]},
    'kalyani': {'swaras': [0, 2, 4, 6, 7, 9, 11, 12]},
    'mohanam': {'swaras': [0, 2, 4, 7, 9, 12]},
    'hindolam': {'swaras': [0, 3, 5, 8, 10, 12]},
    'abheri': {'swaras': [0, 2, 3, 5, 7, 8, 10, 12]}
}
TALAS = ['adi', 'rupakam']

# --- Setup ---
MIDI_OUTPUT_DIR = 'data/midi'
os.makedirs(MIDI_OUTPUT_DIR, exist_ok=True)
BASE_NOTE = 60 # Middle C (Sa)
NOTE_DURATION = 0.25 # Use faster notes for more complex patterns

# --- Musical Pattern Generators ---
def add_notes_to_list(note_list, swaras, current_time, velocity=100):
    """A helper function to add a sequence of notes to a list."""
    for interval in swaras:
        note = pretty_midi.Note(
            velocity=velocity,
            pitch=BASE_NOTE + interval,
            start=current_time,
            end=current_time + NOTE_DURATION
        )
        note_list.append(note)
        current_time += NOTE_DURATION
    return current_time

def generate_scales(swaras, repeats=2):
    """Generates arohanam and avarohanam (ascending/descending scales)."""
    notes = []
    current_time = 0.0
    for _ in range(repeats):
        current_time = add_notes_to_list(notes, swaras[:-1], current_time) # Ascending
        current_time = add_notes_to_list(notes, reversed(swaras), current_time) # Descending
        current_time += NOTE_DURATION # Add a pause
    return notes

def generate_janta_varisai(swaras):
    """Generates paired-note patterns (SS RR GG...)."""
    notes = []
    current_time = 0.0
    for interval in swaras[:-1]:
        current_time = add_notes_to_list(notes, [interval, interval], current_time)
    for interval in reversed(swaras):
        current_time = add_notes_to_list(notes, [interval, interval], current_time)
    return notes

def generate_random_phrases(swaras, num_phrases=12, phrase_length=8):
    """Generates short, random melodic phrases using only the raga's notes."""
    notes = []
    current_time = 0.0
    for _ in range(num_phrases):
        phrase = [random.choice(swaras) for _ in range(phrase_length)]
        current_time = add_notes_to_list(notes, phrase, current_time)
        current_time += NOTE_DURATION * 2 # Pause between phrases
    return notes

def write_midi_file(notes, filename, instrument_name='Sitar'):
    """Writes a list of MIDI notes to a file."""
    if not notes: return
    filepath = os.path.join(MIDI_OUTPUT_DIR, filename)
    midi = pretty_midi.PrettyMIDI()
    instrument = pretty_midi.Instrument(program=pretty_midi.instrument_name_to_program(instrument_name))
    instrument.notes.extend(notes)
    midi.instruments.append(instrument)
    midi.write(filepath)
    print(f"Generated: {filename}")

# --- Main Generation Logic ---
print("--- Generating Final MIDI Dataset ---")
# Clear out any old files to start fresh
for file in os.listdir(MIDI_OUTPUT_DIR):
    os.remove(os.path.join(MIDI_OUTPUT_DIR, file))

# Generate a variety of MIDI files for each raga and tala
for raga, data in RAGAS.items():
    for tala in TALAS:
        # Combine different patterns into one longer file for each combination
        all_patterns = []
        all_patterns.extend(generate_scales(data['swaras']))
        all_patterns.extend(generate_janta_varisai(data['swaras']))
        all_patterns.extend(generate_random_phrases(data['swaras']))

        # Sort all notes by start time to create a coherent piece
        all_patterns.sort(key=lambda note: note.start)

        write_midi_file(all_patterns, f"{raga}_{tala}_composition.mid")

print(f"\n✅ Successfully created your final dataset in '{MIDI_OUTPUT_DIR}'.")
print("You are now ready to preprocess, train, and deploy your model.")

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pretty_midi
import numpy as np
import os
import json
from tqdm.notebook import tqdm
import fluidsynth
from IPython.display import Audio

# --- Configuration ---
# Create directories for our project
os.makedirs('data/midi', exist_ok=True)
os.makedirs('data/processed', exist_ok=True)
os.makedirs('soundfonts', exist_ok=True)
os.makedirs('output/midi', exist_ok=True)
os.makedirs('output/audio', exist_ok=True)
os.makedirs('models', exist_ok=True)

MIDI_DATA_DIR = 'data/midi'
PROCESSED_DATA_DIR = 'data/processed'
MODEL_DIR = 'models'
OUTPUT_DIR = 'output'
SOUNDFONT_PATH = '/content/soundfonts/GeneralUser-GS.sf2' # IMPORTANT: You must upload your own .sf2 file

# Data Processing Config
TIME_RESOLUTION = 100
MAX_VELOCITY_BINS = 32

# Model & Training Config
CONTEXT_LENGTH = 64
BATCH_SIZE = 32
EPOCHS = 20 # Increase for better results
LEARNING_RATE = 0.001
EMBEDDING_DIM = 256
HIDDEN_DIM = 512
NUM_LAYERS = 3

# Set device
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

# IMPORTANT: Create a dummy MIDI file for testing the pipeline
# Your actual files must be named like 'raga_tala_description.mid'
pm = pretty_midi.PrettyMIDI()
instrument = pretty_midi.Instrument(program=0)
instrument.notes.append(pretty_midi.Note(velocity=100, pitch=60, start=0.0, end=0.5))
instrument.notes.append(pretty_midi.Note(velocity=100, pitch=62, start=0.5, end=1.0))
instrument.notes.append(pretty_midi.Note(velocity=100, pitch=64, start=1.0, end=1.5))
pm.instruments.append(instrument)
pm.write(os.path.join(MIDI_DATA_DIR, 'kalyani_adi_test.mid'))
print("Dummy MIDI file 'kalyani_adi_test.mid' created.")
print(f"Please upload your MIDI files to the '{MIDI_DATA_DIR}' directory.")
print(f"Please upload your soundfont file to the '{SOUNDFONT_PATH}' path.")

def get_metadata_from_filename(filename):
    """Extracts Raga and Tala from filename like 'raga_tala_...'.mid"""
    parts = os.path.basename(filename).lower().split('_')
    if len(parts) < 2:
        return "raga_unknown", "tala_unknown"
    return f"raga_{parts[0]}", f"tala_{parts[1]}"

def midi_to_events(midi_file_path):
    """Converts a single MIDI file to a sequence of events."""
    try:
        midi_data = pretty_midi.PrettyMIDI(midi_file_path)
    except Exception:
        return None

    raga, tala = get_metadata_from_filename(midi_file_path)
    events = [raga, tala]

    all_notes = []
    for instrument in midi_data.instruments:
        all_notes.extend(instrument.notes)
    all_notes.sort(key=lambda x: x.start)

    if not all_notes:
        return None

    timed_events = []
    for note in all_notes:
        quantized_velocity = int(note.velocity / 128.0 * (MAX_VELOCITY_BINS - 1))
        timed_events.append((note.start, 'note_on', note.pitch, quantized_velocity))
        timed_events.append((note.end, 'note_off', note.pitch, 0))

    timed_events.sort(key=lambda x: x[0])

    last_event_time = 0.0
    for time, event_type, pitch, velocity in timed_events:
        delta_time = time - last_event_time
        time_steps = int(round(delta_time * TIME_RESOLUTION))

        while time_steps > 0:
            shift_amount = min(time_steps, 100)
            events.append(f'time_shift_{shift_amount}')
            time_steps -= shift_amount

        if event_type == 'note_on':
            events.append(f'velocity_{velocity}')
            events.append(f'note_on_{pitch}')
        else:
            events.append(f'note_off_{pitch}')

        last_event_time = time

    return events

def create_and_save_processed_data():
    """Processes all MIDI files and saves vocabulary and training data."""
    all_events_flat = []
    midi_files = [os.path.join(MIDI_DATA_DIR, f) for f in os.listdir(MIDI_DATA_DIR) if f.endswith(('.mid', '.midi'))]

    print(f"Found {len(midi_files)} MIDI files. Starting preprocessing...")
    for file_path in tqdm(midi_files, desc="Processing MIDI"):
        events = midi_to_events(file_path)
        if events:
            all_events_flat.extend(events)

    if not all_events_flat:
        print("No events were processed. Please check your MIDI files.")
        return

    # Create and save vocabulary
    unique_tokens = sorted(list(set(all_events_flat)))
    token_to_int = {token: i for i, token in enumerate(unique_tokens)}
    int_to_token = {i: token for token, i in token_to_int.items()}
    vocab_path = os.path.join(PROCESSED_DATA_DIR, 'vocab.json')
    with open(vocab_path, 'w') as f:
        json.dump({'token_to_int': token_to_int, 'int_to_token': int_to_token}, f)
    print(f"Vocabulary of size {len(token_to_int)} saved.")

    # Convert all events to integers and save
    encoded_sequence = [token_to_int[token] for token in all_events_flat]
    data_path = os.path.join(PROCESSED_DATA_DIR, 'train_data.npy')
    np.save(data_path, np.array(encoded_sequence, dtype=np.uint16))
    print(f"Encoded sequence of {len(encoded_sequence)} tokens saved.")

# Run the preprocessing
create_and_save_processed_data()

import os
import matplotlib.pyplot as plt
import seaborn as sns
import pretty_midi

MIDI_DATA_DIR = "data/midi"

# List MIDI files
midi_files = [f for f in os.listdir(MIDI_DATA_DIR) if f.endswith(('.mid', '.midi'))]

# Extract Raga & Tala info from filenames
ragas = []
talas = []
for f in midi_files:
    parts = f.split('_')
    if len(parts) >= 2:
        ragas.append(parts[0])
        talas.append(parts[1])

plt.figure(figsize=(10,5))
sns.countplot(x=ragas)
plt.title("Distribution of Ragas in Dataset")
plt.xlabel("Raga Name")
plt.ylabel("Number of MIDI Files")
plt.show()

plt.figure(figsize=(6,4))
sns.countplot(x=talas)
plt.title("Distribution of Talas in Dataset")
plt.xlabel("Tala Name")
plt.ylabel("Number of MIDI Files")
plt.show()

import numpy as np

def get_note_pitches(midi_path):
    midi = pretty_midi.PrettyMIDI(midi_path)
    pitches = []
    for instrument in midi.instruments:
        pitches.extend([note.pitch for note in instrument.notes])
    return pitches

raga_pitch_data = {}
for file in midi_files:
    raga = file.split('_')[0]
    pitches = get_note_pitches(os.path.join(MIDI_DATA_DIR, file))
    raga_pitch_data.setdefault(raga, []).extend(pitches)

plt.figure(figsize=(12,6))
for raga, pitches in raga_pitch_data.items():
    sns.kdeplot(pitches, label=raga, fill=True)
plt.title("Pitch Distribution per Raga")
plt.xlabel("MIDI Note Number (Pitch)")
plt.ylabel("Density")
plt.legend()
plt.show()

class MusicRNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout=0.3):
        super(MusicRNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.dropout = nn.Dropout(dropout)
        self.fc_out = nn.Linear(hidden_size, vocab_size)

    def forward(self, x, hidden=None):
        embedded = self.embedding(x)
        lstm_out, hidden = self.lstm(embedded, hidden)
        out = self.dropout(lstm_out)
        output = self.fc_out(out)
        return output, hidden

class MusicDataset(Dataset):
    def __init__(self, data, context_length):
        self.data = data
        self.context_length = context_length

    def __len__(self):
        return len(self.data) - self.context_length

    def __getitem__(self, idx):
        x = torch.from_numpy(self.data[idx:idx+self.context_length].astype(np.int64))
        y = torch.from_numpy(self.data[idx+1:idx+self.context_length+1].astype(np.int64))
        return x, y

def train_model():
    # Load vocabulary
    with open(os.path.join(PROCESSED_DATA_DIR, 'vocab.json'), 'r') as f:
        vocab = json.load(f)
    vocab_size = len(vocab['token_to_int'])

    # Load data
    data = np.load(os.path.join(PROCESSED_DATA_DIR, 'train_data.npy'))
    if len(data) < CONTEXT_LENGTH + 1:
        print("Not enough data to train. Please add more/longer MIDI files.")
        return

    dataset = MusicDataset(data, CONTEXT_LENGTH)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    # Initialize model, optimizer, and loss
    model = MusicRNN(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(DEVICE)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()

    print(f"Model initialized with {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters.")
    print("Starting training...")

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0

        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        for x, y in pbar:
            x, y = x.to(DEVICE), y.to(DEVICE)

            optimizer.zero_grad()
            output, _ = model(x)

            loss = criterion(output.view(-1, vocab_size), y.view(-1))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()
            pbar.set_postfix({'loss': loss.item()})

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1} complete. Average Loss: {avg_loss:.4f}")

        # Save checkpoint
        checkpoint_path = os.path.join(MODEL_DIR, f'music_rnn_epoch_{epoch+1}.pth')
        torch.save(model.state_dict(), checkpoint_path)

# Run training
train_model()

# 1️⃣ Clean reinstall NumPy and SciPy
!pip uninstall -y numpy scipy
!pip install numpy==1.26.4 scipy==1.11.4 --force-reinstall --no-cache-dir

# 2️⃣ Restart the kernel (very important)
import os
os.kill(os.getpid(), 9)

import torch.nn.functional as F
import scipy.io.wavfile # Move import outside the function


def generate_sequence(model, start_tokens, token_to_int, int_to_token, device, max_len=1024, temperature=1.0, top_k=40):
    model.eval()
    generated_sequence = [token_to_int[token] for token in start_tokens]

    with torch.no_grad():
        pbar = tqdm(range(max_len), desc="Generating tokens")
        for _ in pbar:
            input_tensor = torch.LongTensor([generated_sequence]).to(device)
            output, _ = model(input_tensor)
            logits = output[0, -1, :] / temperature

            # --- START OF THE FIX ---
            # This ensures top_k is never larger than the number of available tokens.
            effective_top_k = min(top_k, logits.shape[-1])
            # --- END OF THE FIX ---

            # Top-k sampling using the new, safer value
            top_k_logits, top_k_indices = torch.topk(logits, effective_top_k)
            probs = F.softmax(top_k_logits, dim=-1)

            next_token_relative_idx = torch.multinomial(probs, 1)
            next_token = top_k_indices[next_token_relative_idx].item()

            generated_sequence.append(next_token)

    return [int_to_token[str(i)] for i in generated_sequence]

def events_to_midi(events, output_path):
    midi = pretty_midi.PrettyMIDI()
    instrument = pretty_midi.Instrument(program=0)

    current_time = 0.0
    current_velocity = 90
    open_notes = {}

    for event in events:
        if 'time_shift' in event:
            current_time += int(event.split('_')[-1]) / TIME_RESOLUTION
        elif 'velocity' in event:
            current_velocity = int(event.split('_')[-1]) * 128 // MAX_VELOCITY_BINS
        elif 'note_on' in event:
            pitch = int(event.split('_')[-1])
            open_notes[pitch] = {'start': current_time, 'velocity': current_velocity}
        elif 'note_off' in event:
            pitch = int(event.split('_')[-1])
            if pitch in open_notes:
                note_info = open_notes.pop(pitch)
                note = pretty_midi.Note(
                    velocity=note_info['velocity'], pitch=pitch,
                    start=note_info['start'], end=current_time
                )
                instrument.notes.append(note)

    midi.instruments.append(instrument)
    midi.write(output_path)
    print(f"MIDI file saved to {output_path}")
    return midi

def render_audio(midi, output_wav_path):
    if not os.path.exists(SOUNDFONT_PATH):
        print(f"Soundfont file not found at '{SOUNDFONT_PATH}'. Cannot render audio.")
        return None
    waveform = midi.fluidsynth(fs=44100, sf2_path=SOUNDFONT_PATH)
    # Note: pretty_midi.fluidsynth returns a float waveform. To save, it needs to be scaled to int16.
    scipy.io.wavfile.write(output_wav_path, 44100, (waveform * 32767).astype(np.int16))
    print(f"Audio rendered to {output_wav_path}")
    return Audio(waveform, rate=44100)

import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import numpy as np
import json
import os
import pretty_midi
from tqdm.notebook import tqdm
from IPython.display import Audio

# --- Configuration (redefined here) ---
# Create directories for our project
os.makedirs('data/midi', exist_ok=True)
os.makedirs('data/processed', exist_ok=True)
os.makedirs('soundfonts', exist_ok=True)
os.makedirs('output/midi', exist_ok=True)
os.makedirs('output/audio', exist_ok=True)
os.makedirs('models', exist_ok=True)

MIDI_DATA_DIR = 'data/midi'
PROCESSED_DATA_DIR = 'data/processed'
MODEL_DIR = 'models'
OUTPUT_DIR = 'output'
SOUNDFONT_PATH = '/content/soundfonts/GeneralUser-GS.sf2' # IMPORTANT: You must upload your own .sf2 file

# Data Processing Config
TIME_RESOLUTION = 100
MAX_VELOCITY_BINS = 32

# Model & Training Config
CONTEXT_LENGTH = 64
BATCH_SIZE = 32
EPOCHS = 20 # Increase for better results
LEARNING_RATE = 0.001
EMBEDDING_DIM = 256
HIDDEN_DIM = 512
NUM_LAYERS = 3

# Set device
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

# IMPORTANT: Create a dummy MIDI file for testing the pipeline
# Your actual files must be named like 'raga_tala_description.mid'
# pm = pretty_midi.PrettyMIDI()
# instrument = pretty_midi.Instrument(program=0)
# instrument.notes.append(pretty_midi.Note(velocity=100, pitch=60, start=0.0, end=0.5))
# instrument.notes.append(pretty_midi.Note(velocity=100, pitch=62, start=0.5, end=1.0))
# instrument.notes.append(pretty_midi.Note(velocity=100, pitch=64, start=1.0, end=1.5))
# pm.instruments.append(instrument)
# pm.write(os.path.join(MIDI_DATA_DIR, 'kalyani_adi_test.mid'))
# print("Dummy MIDI file 'kalyani_adi_test.mid' created.")
print(f"Please upload your MIDI files to the '{MIDI_DATA_DIR}' directory.")
print(f"Please upload your soundfont file to the '{SOUNDFONT_PATH}' path.")

class MusicRNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout=0.3):
        super(MusicRNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.dropout = nn.Dropout(dropout)
        self.fc_out = nn.Linear(hidden_size, vocab_size)

    def forward(self, x, hidden=None):
        embedded = self.embedding(x)
        lstm_out, hidden = self.lstm(embedded, hidden)
        out = self.dropout(lstm_out)
        output = self.fc_out(out)
        return output, hidden

def generate_full_piece(model_checkpoint, raga, tala):
    # Load vocabulary
    with open(os.path.join(PROCESSED_DATA_DIR, 'vocab.json'), 'r') as f:
        vocab = json.load(f)
    token_to_int, int_to_token = vocab['token_to_int'], vocab['int_to_token']
    vocab_size = len(token_to_int)

    # Load model
    model = MusicRNN(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(DEVICE)
    model.load_state_dict(torch.load(model_checkpoint, map_location=DEVICE))

    # Generate
    start_prompt = [f'raga_{raga}', f'tala_{tala}']
    generated_events = generate_sequence(
        model, start_prompt, token_to_int, int_to_token, DEVICE, max_len=1000)

    # Convert and Render
    filename = f"ai_{raga}_{tala}_output"
    midi_path = os.path.join(OUTPUT_DIR, 'midi', f'{filename}.mid')
    audio_path = os.path.join(OUTPUT_DIR, 'audio', f'{filename}.wav')

    generated_midi = events_to_midi(generated_events, midi_path)
    # Note: Audio rendering in the notebook might be unstable.
    # The Streamlit app uses a more robust rendering method.
    return render_audio(generated_midi, audio_path)

# --- Generate a new piece ---
# Make sure to use the correct path to your trained model checkpoint
final_model_path = os.path.join(MODEL_DIR, f'music_rnn_epoch_{EPOCHS}.pth')

if os.path.exists(final_model_path):
    # This will generate a new piece in raga Kalyani and tala Adi
    print("Attempting to generate and render audio in notebook. Note: rendering might be unstable here.")
    display(generate_full_piece(final_model_path, 'mohanam', 'adi')) # Changed raga to 'mohanam'
else:
    print("Trained model not found. Please run the training cell first.")

import torch
import torch.nn as nn
import numpy as np
import json
import os
from torch.utils.data import DataLoader, Dataset

# --- Load vocab and data ---
with open(os.path.join(PROCESSED_DATA_DIR, 'vocab.json'), 'r') as f:
    vocab = json.load(f)
vocab_size = len(vocab['token_to_int'])
data = np.load(os.path.join(PROCESSED_DATA_DIR, 'train_data.npy'))

# --- Dataset for validation ---
class MusicDataset(Dataset):
    def __init__(self, data, context_length):
        self.data = data
        self.context_length = context_length

    def __len__(self):
        return len(self.data) - self.context_length

    def __getitem__(self, idx):
        x = torch.from_numpy(self.data[idx:idx+self.context_length].astype(np.int64))
        y = torch.from_numpy(self.data[idx+1:idx+self.context_length+1].astype(np.int64))
        return x, y

dataset = MusicDataset(data, CONTEXT_LENGTH)
dataloader = DataLoader(dataset, batch_size=64, shuffle=False)

# --- Load trained model ---
model_path = os.path.join(MODEL_DIR, f'music_rnn_epoch_{EPOCHS}.pth')
model = MusicRNN(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(DEVICE)
model.load_state_dict(torch.load(model_path, map_location=DEVICE))
model.eval()

criterion = nn.CrossEntropyLoss()
total_loss = 0
count = 0

with torch.no_grad():
    for x, y in dataloader:
        x, y = x.to(DEVICE), y.to(DEVICE)
        output, _ = model(x)
        loss = criterion(output.view(-1, vocab_size), y.view(-1))
        total_loss += loss.item()
        count += 1

avg_loss = total_loss / count
perplexity = np.exp(avg_loss)
print(f"Validation Loss: {avg_loss:.4f}")
print(f"Perplexity: {perplexity:.2f}")

import matplotlib.pyplot as plt
import seaborn as sns
import pretty_midi
generated_midi_path = 'output/midi/ai_mohanam_adi_output.mid'
real_midi_path = 'data/midi/mohanam_adi_composition.mid'

def get_pitches(midi_file):
    midi = pretty_midi.PrettyMIDI(midi_file)
    return [note.pitch for inst in midi.instruments for note in inst.notes]

plt.figure(figsize=(10,5))
sns.kdeplot(get_pitches(real_midi_path), label="Real Mohanam", fill=True)
sns.kdeplot(get_pitches(generated_midi_path), label="Generated Mohanam", fill=True)
plt.title("Pitch Distribution Comparison")
plt.xlabel("MIDI Pitch")
plt.ylabel("Density")
plt.legend()
plt.show()
